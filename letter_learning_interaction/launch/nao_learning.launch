<launch>
    <arg name="use_robot_in_interaction" default="true" /> <!-- if false, robot-related commands will be skipped -->
    <arg name="nao_ip" default="195.168.1.4" /> <!-- IP address of non-simulated robot (ignored if simulated) -->
    <arg name="use_sim_nao" default="true" /> <!-- if true, webots simulation expected to be running -->

    <arg name="nao_writing" default="$(arg use_robot_in_interaction)" /> <!-- if false, robot will not stand and move its arm to write -->
    <arg name="nao_handedness" default="right" /> 
    <arg name="person_side" default="$(arg nao_handedness)" />   
        
    <arg name="use_external_camera" default="true" /> <!-- if false, camera on robot will be used -->
    <arg name="camera_device" default="/dev/video0" /> <!-- only used if using external camera -->

    <!-- Language of interaction (french and english supported by nao) -->
    <arg name="language" default="english" />
    
    <!-- Name of frame representing writing surface origin (at bottom left of surface, with x to the left and y up) -->
    <arg name="writing_surface_frame_id" default="writing_surface" /> 

    <!-- Where the datasets for generating letter models for the learning algorithm are stored -->
    <arg name="letter_model_dataset_directory" default="default"/> 
    
    <!-- Inputs to learning algorithm -->
    <arg name="shape_feedback_topic" default="shape_feedback" />
    <arg name="words_to_write_topic" default="words_to_write" />
    <arg name="stop_request_topic" default="stop_learning" />    
    <arg name="test_request_topic" default="test_learning" />
    <arg name="touch_info_topic" default="touch_info" />
    <arg name="gesture_info_topic" default="gesture_info" />
    <arg name="shape_writing_finished_topic" default="shape_finished" />
    <arg name="user_drawn_shapes_topic" default="user_drawn_shapes" />
    <arg name="new_teacher_topic" default="new_child" />
    
    <!-- Outputs from learning algorithm -->
    <arg name="trajectory_visualization_topic" default="write_traj" /> 
    <arg name="trajectory_nao_topic" default="write_traj_downsampled" />
    <arg name="clear_writing_surface_topic" default="clear_screen" /> 
    <arg name="camera_publishing_status_topic" default="camera_publishing_status" />
    
    <!-- Frame of camera that will be detecting fiducial markers for words to write -->
    <arg name="word_detector_frame_id" value="camera_frame" if="$(arg use_external_camera)" />
    <arg name="word_detector_frame_id" value="CameraTop_frame" unless="$(arg use_external_camera)" />
    
    <arg name="camera_image_topic" value="v4l/camera/image_raw" if="$(arg use_external_camera)" />
    <arg name="camera_image_topic" value="/nao_camera/image_raw" unless="$(arg use_external_camera)" />

    <rosparam param="/wait_to_sync_traj" subst_value="true">$(arg nao_writing)</rosparam> <!-- if using robot, tablet must wait to display at the same time as it -->
    <arg name="nao_ip_to_use" value="127.0.0.1" if="$(arg use_sim_nao)" /> <!-- look for local simulator -->
    <arg name="nao_ip_to_use" value="$(arg nao_ip)" unless="$(arg use_sim_nao)" />


    <!-- ##### END ARGUMENTS ##### -->  
    <group if="$(arg use_robot_in_interaction)" >
        <group if="$(arg nao_writing)">
            <!-- Start the capabilities for the Nao to trace messages on a writing surface -->
            <include file="$(find nao_trajectory_following)/launch/nao_writing_on_surface.launch" >
                <arg name="use_sim_nao" value="$(arg use_sim_nao)" />
                <arg name="nao_handedness" value="$(arg nao_handedness)" />
                <arg name="nao_ip" value="$(arg nao_ip_to_use)"/>
                <arg name="writing_surface_frame_id" value="$(arg writing_surface_frame_id)" /> 
                <arg name="trajectory_visualization_input_topic" value="$(arg trajectory_visualization_topic)" />
                <arg name="trajectory_nao_input_topic" value="$(arg trajectory_nao_topic)" />
                <arg name="clear_writing_surface_topic" value="$(arg clear_writing_surface_topic)" />
            </include>
        </group>
    </group> 

    <!-- Start the learning algorithm -->
    <node pkg="letter_learning_interaction" type="learning_words_nao.py" name="learning_words_nao">
        
        <param name="use_robot_in_interaction" type="bool" value="$(arg use_robot_in_interaction)"/>
        <param name="language" type="str" value="$(arg language)"/>
        <param name="nao_ip" type="str" value="$(arg nao_ip_to_use)"/>
        <param name="nao_handedness" type="str" value="$(arg nao_handedness)"/>
        <param name="dataset_directory" type="str" value="$(arg letter_model_dataset_directory)" />
        <param name="writing_surface_frame_id" type="str" value="$(arg writing_surface_frame_id)"/>

        <param name="shape_feedback_topic" type="str" value="$(arg shape_feedback_topic)"/>
        <param name="words_to_write_topic" type="str" value="$(arg words_to_write_topic)"/>
        <param name="stop_request_topic" type="str" value="$(arg stop_request_topic)"/>
        <param name="test_request_topic" type="str" value="$(arg test_request_topic)"/>
        <param name="gesture_info_topic" type="str" value="$(arg gesture_info_topic)"/>
        <param name="shape_writing_finished_topic" type="str" value="$(arg shape_writing_finished_topic)"/>
        <param name="user_drawn_shapes_topic" type="str" value="$(arg user_drawn_shapes_topic)"/>
        <param name="new_teacher_topic" type="str" value="$(arg new_teacher_topic)"/>
        
        <param name="trajectory_output_topic" type="str" value="$(arg trajectory_visualization_topic)"/>
        <param name="trajectory_output_nao_topic" type="str" value="$(arg trajectory_nao_topic)"/>        
        <param name="clear_writing_surface_topic" type="str" value="$(arg clear_writing_surface_topic)"/>
        <param name="camera_publishing_status_topic" type="str" value="$(arg camera_publishing_status_topic)"/>
    </node>

    <!-- Start the appropriate camera -->
    <group if="$(arg use_external_camera)">
        <include file="$(find gscam)/v4l.launch" >
            <arg name="DEVICE" value="$(arg camera_device)"/>
        </include>
    </group>
    <group unless="$(arg use_external_camera)">
        <include file="$(find nao_trajectory_following)/launch/nao_camera_controllable.launch">
            <arg name="nao_ip" value="$(arg nao_ip_to_use)"/>
        </include>
    </group>
    
    <!-- Start the card detection -->
    <include file="$(find ros_markers)/detect.launch" >
        <arg name="camera_frame_id" value="$(arg word_detector_frame_id)"/>
        <arg name="image_topic" value="$(arg camera_image_topic)"/>
    </include>
    <node pkg="letter_learning_interaction" type="word_card_detector.py" name="word_detector">
        <param name="detector_frame_id" type="str" value="$(arg word_detector_frame_id)"/>
        <param name="language" type="str" value="$(arg language)"/>
        <param name="detected_words_topic" type="str" value="$(arg words_to_write_topic)"/>
        <param name="stop_request_topic" type="str" value="$(arg stop_request_topic)"/>
        <param name="test_request_topic" type="str" value="$(arg test_request_topic)"/>
    </node>

    <!-- Start the display manager server, which will be accessed by the gesture_manager node and the relevant shape_learning package's node -->
    <node pkg="letter_learning_interaction" type="display_manager_server.py" name="display_manager_server"/>

    <!-- Start the gesture listener, which will map gestures to the relevant shape -->
    <node pkg="letter_learning_interaction" type="tablet_input_interpreter.py" name="tablet_input_interpreter">   
        <param name="shape_feedback_topic" type="str" value="$(arg shape_feedback_topic)"/>
        <param name="touch_info_topic" type="str" value="$(arg touch_info_topic)"/>
        <param name="gesture_info_topic" type="str" value="$(arg gesture_info_topic)"/>
    </node>

</launch>
